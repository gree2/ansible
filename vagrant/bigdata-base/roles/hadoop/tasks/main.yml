---
# tasks file for hadoop

- name: create user hadoop
  user: >
    name={{ user.hadoop }}
    password={{ user.hadoop_password }}
    generate_ssh_key=yes
    ssh_key_bits=2048
    ssh_key_file=.ssh/id_rsa

- name: ensure the /home/hadoop/.ssh directory exists
  file: >
    path=/home/{{ user.hadoop }}/.ssh
    state=directory

# - name: tell the host about our servers it might want to ssh to
#   become: yes
#   become_user: "{{ user.hadoop }}"
#   known_hosts: >
#     path='/home/{{ user.hadoop }}/.ssh/known_hosts'
#     name='singlenode'
#     key="{{ lookup('file', '/home/{{ user.hadoop }}/.ssh/id_rsa.pub') }}"

- name: add to authorized_keys
  shell: "cat /home/{{ user.hadoop }}/.ssh/id_rsa.pub >> /home/{{ user.hadoop }}/.ssh/authorized_keys"
  become: yes
  become_user: "{{ user.hadoop }}"

- name: create /app/hadoop
  file: >
    path={{ dir.app_base }}
    state=directory
    owner={{ user.hadoop }}
    group={{ user.hadoop_group }}
    recurse=yes

- name: create /opt/bigdata
  file: >
    path={{ dir.install_base }}
    state=directory
    owner={{ user.hadoop }}
    group={{ user.hadoop_group }}
    recurse=yes

- name: unzip hadoop tar ball
  unarchive: >
    src=/software/{{ version.hadoop }}.tar.gz
    dest={{ dir.install_base }}
    copy=no
    owner={{ user.hadoop }}
    group={{ user.hadoop_group }}

- name: hadoop symbolic link
  file: >
    src={{ dir.install_base }}/{{ version.hadoop }}
    dest={{ dir.install_base }}/hadoop
    state=link
    owner={{ user.hadoop }}
    group={{ user.hadoop_group }}

- name: hadoop conf
  template: >
    src={{ item }}.j2
    dest={{ dir.hadoop_conf }}/{{ item }}
    owner={{ user.hadoop }}
    group={{ user.hadoop_group }}
  with_items:
    - core-site.xml
    - hadoop-env.sh
    - hdfs-site.xml
    - mapred-site.xml
    - slaves
    - yarn-site.xml

- name: copy .bashrc
  copy: >
    src=.bashrc
    dest=/home/{{ user.hadoop }}/.bashrc
    owner={{ user.hadoop }}
    group={{ user.hadoop_group }}

- name: source .bashrc
  shell: . /home/{{ user.hadoop }}/.bashrc
  become: yes
  become_user: "{{ user.hadoop }}"

# - name: format and start nn dn daemon
#   # sudo: true
#   # sudo_user: "{{ user.hadoop }}"
#   shell: creates={{ dir.install_base }}/hadoop/{{ version.hadoop }} su - {{ user.hadoop }} -c "hdfs namenode -format -force"
#   shell: start-dfs.sh
#   shell: hdfs dfs -ls /
#   become: yes
#   become_user: "{{ user.hadoop }}"
