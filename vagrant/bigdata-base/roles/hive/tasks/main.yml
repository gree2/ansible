---
# tasks file for hive

- name: unzip hive tar ball
  unarchive:
    src: "/software/{{ version.hive }}.tar.gz"
    dest: "{{ dir.install_base }}"
    copy: no
    owner: "{{ user.hadoop }}"
    group: "{{ user.hadoop_group }}"

- name: chown hive install folder
  file: 
    path: "{{ dir.install_base }}/{{ version.hive }}"
    owner: "{{ user.hadoop }}"
    group: "{{ user.hadoop_group }}"

- name: hive symbolic link
  file:
    src: "{{ dir.install_base }}/{{ version.hive }}"
    dest: "{{ dir.install_base }}/hive"
    state: link
    owner: "{{ user.hadoop }}"
    group: "{{ user.hadoop_group }}"

- name: copy mysql connector
  command: "cp '/software/{{ version.mysql_connector }}.jar' '{{ dir.hive_lib }}'"

- name: hive conf
  template:
    src: "{{ item }}.j2"
    dest: "{{ dir.hive_conf }}/{{ item }}"
    owner: "{{ user.hadoop }}"
    group: "{{ user.hadoop_group }}"
  with_items:
    - beeline-log4j.properties
    - hive-env.sh
    - hive-exec-log4j.properties
    - hive-log4j.properties
    - hive-site.xml

- name: create /app/hive
  file: >
    path={{ dir.app_hive }}
    state=directory
    owner={{ user.hadoop }}
    group={{ user.hadoop_group }}
    recurse=yes

- name: copy .bashrc
  template:
    src: .bashrc
    dest: /home/{{ user.hadoop }}/.bashrc
    owner: "{{ user.hadoop }}"
    group: "{{ user.hadoop_group }}"

- name: source .bashrc
  shell: . /home/{{ user.hadoop }}/.bashrc
  become: yes
  become_user: "{{ user.hadoop }}"

- copy:
    src: "{{ item }}"
    dest: "/home/{{ user.hadoop }}"
    owner: "{{ user.hadoop }}"
    group: "{{ user.hadoop_group }}"
  with_items:
    - employee.create
    - employee.data
    - employee.load
